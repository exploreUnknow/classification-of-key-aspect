{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "构建字典\n"
     ]
    }
   ],
   "source": [
    "#这个文件用来测试功能，弃用\n",
    "import csv\n",
    "path_all_data = \"/home/user/Programs/PMA_lihang/data/allitems_v5.csv\"\n",
    "f_all_data = open(path_all_data, 'r', encoding='utf-8')\n",
    "f_reader = csv.reader(f_all_data)\n",
    "#构建字典\n",
    "dict_all_data = {}\n",
    "ii = 0\n",
    "print(\"构建字典\")\n",
    "for line in f_reader:\n",
    "    data = \"\"\n",
    "    for i in range(len(line)):\n",
    "        data += line[i]\n",
    "        if i != len(line) - 1:\n",
    "            data += ','\n",
    "    # print(data)\n",
    "    key_value = data.split('\\t##=divide=##\\t')\n",
    "    dict_all_data[key_value[0]] = key_value[1:]\n",
    "\n",
    "    # print(dict_all_data[key_value[0]])\n",
    "    # ii += 1\n",
    "    # if ii > 25:\n",
    "    #     break\n",
    "# print(dict_all_data)\n",
    "#关闭文件\n",
    "f_all_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_all_data = \"/home/user/Programs/PMA_lihang/data/allitems_v5.csv\"\n",
    "#数据来自下面三个文件\n",
    "path_data1 = \"/home/user/Programs/PMA_lihang/data/pt1_file2_v2.csv\"\n",
    "path_data2 = \"/home/user/Programs/PMA_lihang/data/pt2_file4_v2.csv\"\n",
    "path_data3 = \"/home/user/Programs/PMA_lihang/data/pt3_file6_v2.csv\"\n",
    "\n",
    "path_context_v_type = \"/home/user/Programs/PMA_lihang/mypaper/data/data_context_v_type.csv\"\n",
    "path_context_root_cause = \"/home/user/Programs/PMA_lihang/mypaper/data/data_context_root_cause.csv\"\n",
    "path_context_att_vec = \"/home/user/Programs/PMA_lihang/mypaper/data/data_context_att_vec.csv\"\n",
    "path_context_att_type = \"/home/user/Programs/PMA_lihang/mypaper/data/data_context_att_type.csv\"\n",
    "# 读取数据\n",
    "import csv\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "f_data1 = open(path_data1, 'r', encoding='utf-8')\n",
    "f_data2 = open(path_data2, 'r', encoding='utf-8')\n",
    "f_data3 = open(path_data3, 'r', encoding='utf-8')\n",
    "\n",
    "f_v_type = open(path_context_v_type, 'a', encoding='utf-8')\n",
    "f_root_cause = open(path_context_root_cause, 'a', encoding='utf-8')\n",
    "f_att_vec = open(path_context_att_vec, 'a', encoding='utf-8')\n",
    "f_att_type = open(path_context_att_type, 'a', encoding='utf-8')\n",
    "\n",
    "\n",
    "\n",
    "# f_all_data = open(path_all_data, 'r', encoding='utf-8')\n",
    "# f_reader = csv.reader(f_all_data)\n",
    "# #构建字典\n",
    "# dict_all_data = {}\n",
    "# ii = 0\n",
    "# print(\"构建字典\")\n",
    "# for line in tqdm(f_reader):\n",
    "#     data = \"\"\n",
    "#     for i in range(len(line)):\n",
    "#         data += line[i]\n",
    "#         if i != len(line) - 1:\n",
    "#             data += ','\n",
    "#     # print(data)\n",
    "#     key_value = data.split('\\t##=divide=##\\t')\n",
    "#     dict_all_data[key_value[0]] = key_value[1:]\n",
    "\n",
    "#     # print(dict_all_data[key_value[0]])\n",
    "#     # ii += 1\n",
    "#     # if ii > 25:\n",
    "#     #     break\n",
    "# # print(dict_all_data)\n",
    "# #关闭文件\n",
    "# f_all_data.close()\n",
    "\n",
    "def write_to_file(f_reader1):\n",
    "    for line in f_reader1:\n",
    "        # print(line)\n",
    "        # break\n",
    "        data_v_type = []\n",
    "        data_root_cause = []\n",
    "        data_att_vec = []\n",
    "        data_att_type = []\n",
    "\n",
    "        if line[6] != '':\n",
    "            #print(\"*\")\n",
    "            data_v_type.append(line[0])\n",
    "            data_v_type.append(line[6])\n",
    "            data_v_type.append(dict_all_data[line[0]][0])\n",
    "            #列表写入csv文件\n",
    "            writer = csv.writer(f_v_type)\n",
    "            writer.writerow(data_v_type)\n",
    "\n",
    "\n",
    "        \n",
    "        if line[5] != '':\n",
    "            #print(\"#\")\n",
    "            data_root_cause.append(line[0])\n",
    "            data_root_cause.append(line[5])\n",
    "            data_root_cause.append(dict_all_data[line[0]][0])\n",
    "            #写入文件\n",
    "            writer = csv.writer(f_root_cause)\n",
    "            writer.writerow(data_root_cause)\n",
    "        \n",
    "        if line[4] != '':\n",
    "            #print(\"$\")\n",
    "            data_att_vec.append(line[0])\n",
    "            data_att_vec.append(line[4])\n",
    "            data_att_vec.append(dict_all_data[line[0]][0])\n",
    "            #写入文件\n",
    "            writer = csv.writer(f_att_vec)\n",
    "            writer.writerow(data_att_vec)\n",
    "        \n",
    "        if line[2] != '':\n",
    "            #print(\"%\")\n",
    "            data_att_type.append(line[0])\n",
    "            data_att_type.append(line[2])\n",
    "            data_att_type.append(dict_all_data[line[0]][0])\n",
    "            #写入文件\n",
    "            writer = csv.writer(f_att_type)\n",
    "            writer.writerow(data_att_type)\n",
    "\n",
    "f_reader1 = csv.reader(f_data1)\n",
    "write_to_file(f_reader1)\n",
    "f_reader1 = csv.reader(f_data2)\n",
    "write_to_file(f_reader1)\n",
    "f_reader1 = csv.reader(f_data3)\n",
    "write_to_file(f_reader1)\n",
    "\n",
    "    \n",
    "\n",
    "# f_reader1 = csv.reader(f_data2)\n",
    "# for line in f_reader1:\n",
    "#     print(line[0])\n",
    "    \n",
    "#     break\n",
    "\n",
    "\n",
    "# f_reader1 = csv.reader(f_data3)\n",
    "# for line in f_reader1:\n",
    "#     print(line[0])\n",
    "    \n",
    "#     break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据来自下面三个文件\n",
    "path_data1 = \"/home/user/Programs/PMA_lihang/data/pt1_file2_v2.csv\"\n",
    "path_data2 = \"/home/user/Programs/PMA_lihang/data/pt2_file4_v2.csv\"\n",
    "path_data3 = \"/home/user/Programs/PMA_lihang/data/pt3_file6_v2.csv\"\n",
    "\n",
    "path_context_impact = \"/home/user/Programs/PMA_lihang/mypaper/data/data_context_impact.csv\"\n",
    "f_impact = open(path_context_impact, 'a', encoding='utf-8')\n",
    "import csv\n",
    "f_data1 = open(path_data1, 'r', encoding='utf-8')\n",
    "f_data2 = open(path_data2, 'r', encoding='utf-8')\n",
    "f_data3 = open(path_data3, 'r', encoding='utf-8')\n",
    "\n",
    "def write_to_file(f_reader1):\n",
    "    for line in f_reader1:\n",
    "        \n",
    "        if line[3] != '':\n",
    "            data_impact = []\n",
    "            data_impact.append(line[0])\n",
    "            data_impact.append(line[3])\n",
    "            data_impact.append(dict_all_data[line[0]][0])\n",
    "            #写入文件\n",
    "            writer = csv.writer(f_impact)\n",
    "            writer.writerow(data_impact)\n",
    "\n",
    "\n",
    "f_reader1 = csv.reader(f_data1)\n",
    "write_to_file(f_reader1)\n",
    "f_reader1 = csv.reader(f_data2)\n",
    "write_to_file(f_reader1)\n",
    "f_reader1 = csv.reader(f_data3)\n",
    "write_to_file(f_reader1)\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将短语从上下文中删除\n",
    "path = '/home/user/Programs/PMA_lihang/mypaper/data/data_context_v_type.csv'\n",
    "path1 = \"/home/user/Programs/PMA_lihang/mypaper/data/data_context_v_type_.csv\"\n",
    "import csv\n",
    "\n",
    "f = open(path, 'r', encoding='utf-8')\n",
    "f1 = open(path1, 'a', encoding='utf-8')\n",
    "\n",
    "f_reader = csv.reader(f)\n",
    "for line in f_reader:\n",
    "    data = []\n",
    "    data.append(line[0])\n",
    "    data.append(line[1])\n",
    "    head_tail = line[2].split(line[1])\n",
    "    description = \"\"\n",
    "    for i in range(len(head_tail)):\n",
    "        description += head_tail[i]\n",
    "    data.append(description)\n",
    "    #写入文件\n",
    "    writer = csv.writer(f1)\n",
    "    writer.writerow(data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/user/Programs/PMA_lihang/mypaper/module/bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 23, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "\n",
    "# path = \"/home/user/Programs/PMA_lihang/mypaper/module/bert-base-uncased\"\n",
    "# #加载bert模型\n",
    "# model = BertModel.from_pretrained(path)\n",
    "# #加载分词器\n",
    "# tokenizer = BertTokenizer.from_pretrained(path)\n",
    "\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "\n",
    "text = \"Buffer overflow Buffer overflow Buffer overflow Buffer overflow Buffer overflow Buffer overflow Buffer overflow\"#\"Buffer overflow, in Solaris x86 mkcookie allows local users to obtain root access.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def get_sentence_embedding(text):\n",
    "#     # tokenize text and convert tokens to ids\n",
    "#     tokens = tokenizer.tokenize(text) # ['i', 'like', 'strawberries']\n",
    "#     input_ids = tokenizer.convert_tokens_to_ids(tokens) # [1045, 2066, 13137]\n",
    "\n",
    "#     # add special tokens [CLS] and [SEP]\n",
    "#     input_ids = [tokenizer.cls_token_id] + input_ids + [tokenizer.sep_token_id] \n",
    "#         # [101, 1045, 2066, 13137, 102]\n",
    "\n",
    "#     # convert input ids to tensor\n",
    "#     input_tensor = torch.tensor([input_ids]) # shape (1,5)\n",
    "\n",
    "#     # feed input tensor to BERT model\n",
    "#     with torch.no_grad():\n",
    "#         output = model(input_tensor)\n",
    "\n",
    "    \n",
    "#     # get the last hidden states of BERT\n",
    "#     last_hidden_state = output.last_hidden_state # shape (1,5,dim)\n",
    "#     print(last_hidden_state.shape)\n",
    "#     # get the sentence embedding by taking the first token ([CLS])\n",
    "#     sentence_embedding = last_hidden_state[:,0,:] # shape (1,dim)\n",
    "#     return sentence_embedding\n",
    "\n",
    "\n",
    "path_v_type_description = \"/home/user/Programs/PMA_lihang/mypaper/label_description/v_type.txt\"\n",
    "f_v_type = open(path_v_type_description, 'r', encoding='utf-8')\n",
    "f_v_type_reader = f_v_type.readlines()\n",
    "v_type_description = []\n",
    "for line in f_v_type_reader:\n",
    "    v_type_description.append(line.strip())\n",
    "\n",
    "# for i in range(len(v_type_description) - 1):\n",
    "#     print(v_type_description[i])\n",
    "\n",
    "#计算两个tensor的余弦相似度，shape是(1,768)\n",
    "def cos_sim(a, b):\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    cos_sim = cos(a, b)\n",
    "    return cos_sim\n",
    "\n",
    "#计算两个tensor的欧式距离，shape是(1,768)\n",
    "def euclidean_dist(a, b):\n",
    "    dist = nn.PairwiseDistance(p=2)\n",
    "    dist = dist(a, b)\n",
    "    return dist\n",
    "\n",
    "#计算两个tensor的曼哈顿距离，shape是(1,768)\n",
    "def manhattan_dist(a, b):\n",
    "    dist = nn.PairwiseDistance(p=1)\n",
    "    dist = dist(a, b)\n",
    "    return dist\n",
    "\n",
    "#定义\n",
    "# a = get_sentence_embedding(text)\n",
    "# for i in v_type_description:\n",
    "#     b = get_sentence_embedding(i)\n",
    "#     print(cos_sim(a, b))\n",
    "\n",
    "# a = get_sentence_embedding(text)\n",
    "# for i in v_type_description:\n",
    "#     b = get_sentence_embedding(i)\n",
    "#     print(euclidean_dist(a, b))\n",
    "\n",
    "# a = get_sentence_embedding(text)\n",
    "# for i in v_type_description:\n",
    "#     b = get_sentence_embedding(i)\n",
    "#     print(manhattan_dist(a, b))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载模型太慢了。。。单独拿出来\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import gensim\n",
    "\n",
    "\n",
    "# path = \"/home/user/Programs/PMA_lihang/mypaper/module/bert_large_uncased\"\n",
    "# #加载bert模型\n",
    "# model = BertModel.from_pretrained(path)\n",
    "# #加载分词器\n",
    "# tokenizer = BertTokenizer.from_pretrained(path)\n",
    "\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    " \n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\"/home/user/Programs/PMA_lihang/code_lihang/word2vec/model/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "# model = Word2Vec.load(\"/home/user/Programs/PMA_lihang/code_lihang/word2vec/model/word2vec.model\")\n",
    "#加载谷歌新闻预训练的word2vec模型\n",
    "# model = Word2Vec.load(\"/home/user/Programs/PMA_lihang/code_lihang/word2vec/model/GoogleNews-vectors-negative300.bin\")\n",
    "# model = Word2Vec.load(\"/home/user/Programs/PMA_lihang/code_lihang/word2vec/model/GoogleNews-vectors-negative300.bin\", binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import gensim\n",
    "\n",
    "\n",
    "# path = \"/home/user/Programs/PMA_lihang/mypaper/module/bert-base-uncased\"\n",
    "# #加载bert模型\n",
    "# model = BertModel.from_pretrained(path)\n",
    "# #加载分词器\n",
    "# tokenizer = BertTokenizer.from_pretrained(path)\n",
    "\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    " \n",
    "# model = gensim.models.KeyedVectors.load_word2vec_format(\"/home/user/Programs/PMA_lihang/code_lihang/word2vec/model/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "# model = Word2Vec.load(\"/home/user/Programs/PMA_lihang/code_lihang/word2vec/model/word2vec.model\")\n",
    "#加载谷歌新闻预训练的word2vec模型\n",
    "# model = Word2Vec.load(\"/home/user/Programs/PMA_lihang/code_lihang/word2vec/model/GoogleNews-vectors-negative300.bin\")\n",
    "# model = Word2Vec.load(\"/home/user/Programs/PMA_lihang/code_lihang/word2vec/model/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "from gensim.parsing.preprocessing import STOPWORDS \n",
    "stop_words = STOPWORDS\n",
    "\n",
    "\n",
    "def deal_with_text(v_type):\n",
    "    v_type_ = []\n",
    "    for i in range(len(v_type)):\n",
    "        content = v_type[i]\n",
    "        content = re.sub(\"[^\\u4e00-\\u9fa5^a-z^A-Z^0-9]\", \" \", str(content))\n",
    "        # 只保留数字和字母\n",
    "        content = re.sub(\"[^a-z^A-Z^0-9]\", \" \", str(content))\n",
    "        #去除连续的空格\n",
    "        content = re.sub(\"\\s+\", \" \", content)\n",
    "        #删除回车符\n",
    "        content = re.sub(\"\\n\", \" \", content)\n",
    "        #删除开头和结尾的空格\n",
    "        content = content.strip()\n",
    "        #print(content)\n",
    "        content = content.split(\" \")\n",
    "        #将所有大写字母转换为小写\n",
    "        content = [word.lower() for word in content]\n",
    "        for j in range(len(content)):\n",
    "            v_type_.append(content[j])\n",
    "            # print(\"处理后的文本：\")\n",
    "            # print(content[j])\n",
    "        # print(v_type_)\n",
    "\n",
    "         \n",
    "        # 定义一个文本字符串 \n",
    "        # text = \"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and fishery rihgts at once.\" \n",
    "        # # 对文本进行分词（默认使用空格分隔）\n",
    "        # tokens = text.split()\n",
    "        # # 遍历分词结果，删除在停用词列表中的词 \n",
    "        # filtered_tokens = [token for token in v_type_ if token not in stop_words] \n",
    "        # # 将过滤后的分词结果重新拼接成字符串 \n",
    "        \n",
    "        # filtered_text = ' '.join(filtered_tokens)\n",
    "        # v_type_ = filtered_text.split(\" \")\n",
    "        # print(v_type_)\n",
    "    return v_type_\n",
    "\n",
    "\n",
    "\n",
    "def get_sentence_embedding(text, text2):\n",
    "    # tokenize text and convert tokens to ids\n",
    "    tokens = tokenizer.tokenize(text) # ['i', 'like', 'strawberries']\n",
    "    print(tokens)\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens) # [1045, 2066, 13137]\n",
    "\n",
    "    tokens2 = tokenizer.tokenize(text2) # ['i', 'like', 'strawberries']\n",
    "    print(tokens2)\n",
    "    input_ids2 = tokenizer.convert_tokens_to_ids(tokens2) # [1045, 2066, 13137]\n",
    "    \n",
    "    # print(input_ids)\n",
    "    # print(input_ids2)\n",
    "\n",
    "    # add special tokens [CLS] and [SEP]\n",
    "    input_ids = [tokenizer.cls_token_id] + input_ids + [tokenizer.sep_token_id] + input_ids2 + [tokenizer.sep_token_id] # [101, 1045, 2066, 13137, 102]\n",
    "        # [101, 1045, 2066, 13137, 102]\n",
    "\n",
    "    # convert input ids to tensor\n",
    "    input_tensor = torch.tensor([input_ids]) # shape (1,5)\n",
    "\n",
    "    # print(input_tensor.shape)   \n",
    "    # feed input tensor to BERT model\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "\n",
    "    \n",
    "    # get the last hidden states of BERT\n",
    "    last_hidden_state = output.last_hidden_state # shape (1,5,dim)\n",
    "    # print(last_hidden_state.shape)\n",
    "    # get the sentence embedding by taking the first token ([CLS])\n",
    "    sentence_embedding = last_hidden_state#[:,0,:] # shape (1,dim)\n",
    "    # print(sentence_embedding.shape)\n",
    "    return sentence_embedding\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(len(v_type_description) - 1):\n",
    "#     print(v_type_description[i])\n",
    "\n",
    "#计算两个tensor的余弦相似度，shape是(1,768)\n",
    "def cos_sim(a, b):\n",
    "    cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "    cos_sim = cos(a, b)\n",
    "    return cos_sim\n",
    "\n",
    "#计算两个tensor的欧式距离，shape是(1,768)\n",
    "def euclidean_dist(a, b):\n",
    "    dist = nn.PairwiseDistance(p=2)\n",
    "    dist = dist(a, b)\n",
    "    return dist\n",
    "\n",
    "#计算两个tensor的曼哈顿距离，shape是(1,768)\n",
    "def manhattan_dist(a, b):\n",
    "    dist = nn.PairwiseDistance(p=1)\n",
    "    dist = dist(a, b)\n",
    "    return dist\n",
    "\n",
    "#定义\n",
    "# a = get_sentence_embedding(text)\n",
    "# for i in v_type_description:\n",
    "#     b = get_sentence_embedding(i)\n",
    "#     print(cos_sim(a, b))\n",
    "\n",
    "# a = get_sentence_embedding(text)\n",
    "# for i in v_type_description:\n",
    "#     b = get_sentence_embedding(i)\n",
    "#     print(euclidean_dist(a, b))\n",
    "\n",
    "# a = get_sentence_embedding(text)\n",
    "# for i in v_type_description:\n",
    "#     b = get_sentence_embedding(i)\n",
    "#     print(manhattan_dist(a, b))\n",
    "\n",
    "#自己训练的word2vec\n",
    "def get_vectorOOV1(s):\n",
    "  try:\n",
    "    return np.array(model.wv.get_vector(s))\n",
    "  except KeyError:\n",
    "    #print(\"$$$$$$$\")\n",
    "    return np.random.random((300,))\n",
    "\n",
    "def get_vector(v_type_):\n",
    "    v_type_vec=[]\n",
    "    for i in range(len(v_type_)):\n",
    "        temp = []\n",
    "        #for j in range(len(v_type_[i])):\n",
    "        temp.append(get_vectorOOV1(v_type_[i]))\n",
    "        # for j in range(10-len(v_type_[i])):\n",
    "        #     temp.append(np.random.random((300,)))\n",
    "        v_type_vec.append(temp)\n",
    "    return v_type_vec\n",
    "\n",
    "#新闻word2vec\n",
    "def get_vectorOOV2(s):\n",
    "  try:\n",
    "    return np.array(model.get_vector(s))\n",
    "  except KeyError:\n",
    "    #print(\"$$$$$$$\")\n",
    "    return np.random.random((300,))\n",
    "\n",
    "def get_vector2(v_type_):\n",
    "    v_type_vec=[]\n",
    "    for i in range(len(v_type_)):\n",
    "        temp = []\n",
    "        #for j in range(len(v_type_[i])):\n",
    "        temp.append(get_vectorOOV2(v_type_[i]))\n",
    "        # for j in range(10-len(v_type_[i])):\n",
    "        #     temp.append(np.random.random((300,)))\n",
    "        v_type_vec.append(temp)\n",
    "    return v_type_vec\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用word2vec获取影响类别\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2833, dtype=torch.float64)\n",
      "tensor(0.4062, dtype=torch.float64)\n",
      "tensor(0.5262, dtype=torch.float64)\n",
      "tensor(0.2459, dtype=torch.float64)\n",
      "tensor(0.2218, dtype=torch.float64)\n",
      "tensor(0.3561, dtype=torch.float64)\n",
      "tensor(0.2481, dtype=torch.float64)\n",
      "tensor(0.5708, dtype=torch.float64)\n",
      "tensor(0.2792, dtype=torch.float64)\n",
      "tensor(0.3083, dtype=torch.float64)\n",
      "tensor(0.1835, dtype=torch.float64)\n",
      "tensor(0.4272, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#用word2vec模型\n",
    "# 导入PorterStemmer和word_tokenize\n",
    "\n",
    "\n",
    "from gensim.parsing.preprocessing import STOPWORDS \n",
    "stop_words = STOPWORDS \n",
    "# 定义一个文本字符串 \n",
    "def filter(tokens):\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words] \n",
    "# 将过滤后的分词结果重新拼接成字符串 \n",
    "    return filtered_tokens\n",
    "# text = \"Buffer overflow Buffer overflow Buffer overflow Buffer overflow Buffer overflow Buffer overflow Buffer overflow\"#\"Buffer overflow, in Solaris x86 mkcookie allows local users to obtain root access.\"\n",
    "text = \"Buffer overflow, in Solaris x86 mkcookie allows local users to obtain root access.\"\n",
    "\n",
    "# text = \"an attacker, A flaw was found in the hivex library. This flaw allows to input a specially crafted Windows Registry (hive) file, which would cause hivex to recursively call the _get_children() function, leading to a stack overflow. The highest threat from this vulnerability is to system availability.\"\n",
    "# text = \"A  attacker,An out-of-bounds write issue was addressed with improved bounds checking. This issue is fixed in macOS Monterey 12.3. A remote attacker may be able to cause unexpected system termination or corrupt kernel memory.\"\n",
    "#text = \"A attacker ,A logic issue was addressed with improved state management. This issue is fixed in iTunes 12.12.4 for Windows. A attacker may be able to elevate their privileges.\"\n",
    "# tetx = \"an attacker, ASP.NET Core 2.0 allows to steal log-in session information such as cookies or authentication tokens via a specially crafted URL aka \"\"ASP.NET Core Elevation Of Privilege Vulnerability\"\".\"\n",
    "\n",
    "# text = \"via a UDP packet, resulting in Denial of Service.handle_ipDefaultTTL in agent/mibgroup/ip-mib/ip_scalars.c in Net-SNMP 5.8 through 5.9.3 has a NULL Pointer Exception bug that can be used by a remote attacker (who has write access) to cause the instance to crash via a crafted UDP packet, resulting in Denial of Service.\"\n",
    "\n",
    "text = text.split(\" \")\n",
    "text = deal_with_text(text)\n",
    "text = filter(text)\n",
    "# print(text)\n",
    "a = get_vector2(text)\n",
    "#list转tensor\n",
    "# print(len(a[0][0]))\n",
    "a = torch.tensor(a)\n",
    "#把第二维度删除\n",
    "a = torch.squeeze(a)\n",
    "#取平均值\n",
    "\n",
    "# a = torch.mean(a, 0)\n",
    "a = torch.mean(a[:3], 0 )*0.8 + torch.mean(a[3:], 0)*0.2\n",
    "# print(a.shape)\n",
    "\n",
    "\n",
    "path_v_type_description = \"/home/user/Programs/PMA_lihang/mypaper/label_description/v_type.txt\"#v_type.txt\"\n",
    "f_v_type = open(path_v_type_description, 'r', encoding='utf-8')\n",
    "f_v_type_reader = f_v_type.readlines()\n",
    "v_type_description = []\n",
    "for line in f_v_type_reader:\n",
    "    v_type_description.append(line.strip())\n",
    "\n",
    "for i in v_type_description:\n",
    "    # print(i)\n",
    "    text = i.split(\" \")\n",
    "    text = deal_with_text(text)\n",
    "    text = filter(text)\n",
    "    # print(text)\n",
    "    # a = get_vector(text)\n",
    "    b = get_vector2(text)\n",
    "    \n",
    "    \n",
    "    #list转tensor\n",
    "    b = torch.tensor(b)\n",
    "    #把第二维度删除\n",
    "    b = torch.squeeze(b)\n",
    "    \n",
    "    #取平均值\n",
    "    # b = torch.mean(b, 0)\n",
    "    # 取加权平均值，长度的前20%的token有80%的权重，长度的后80%的token有20%的权重\n",
    "    # b = torch.mean(b[0:int(len(b)*0.2)], 0) * 0.8 + torch.mean(b[int(len(b)*0.2):len(b)], 0) * 0.2\n",
    "    #\n",
    "    b = torch.mean(b[:5], 0 )*0.8 + torch.mean(b[5:], 0)*0.2\n",
    "    # print(b.shape)\n",
    "    print(cos_sim(a, b))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "via internal API endpoints.,\n",
      "The iot-manager microservice 1.0.0 in Northern.tech Mender Enterprise before 3.2.2 allows SSRF because the Azure IoT Hub integration provides several SSRF primitives that can execute cross-tenant actions via internal API endpoints.\n",
      "['via', 'internal', 'api', 'end', '##points', '.', ',']\n",
      "['the', 'io', '##t', '-', 'manager', 'micro', '##ser', '##vic', '##e', '1', '.', '0', '.', '0', 'in', 'northern', '.', 'tech', 'men', '##der', 'enterprise', 'before', '3', '.', '2', '.', '2', 'allows', 'ssr', '##f', 'because', 'the', 'azure', 'io', '##t', 'hub', 'integration', 'provides', 'several', 'ssr', '##f', 'primitive', '##s', 'that', 'can', 'execute', 'cross', '-', 'tenant', 'actions', 'via', 'internal', 'api', 'end', '##points', '.']\n",
      "via field argument or parameter\n",
      "['via', 'field', 'argument', 'or', 'parameter']\n",
      "[]\n",
      "tensor(0.0087)\n",
      "Via some crafted data\n",
      "['via', 'some', 'crafted', 'data']\n",
      "[]\n",
      "tensor(0.0124)\n",
      "By executing the script\n",
      "['by', 'executing', 'the', 'script']\n",
      "[]\n",
      "tensor(0.0069)\n",
      "HTTP protocol correlation\n",
      "['http', 'protocol', 'correlation']\n",
      "[]\n",
      "tensor(0.0016)\n",
      "Call API\n",
      "['call', 'api']\n",
      "[]\n",
      "tensor(-0.0026)\n"
     ]
    }
   ],
   "source": [
    "#用bert模型专门做att_vec\n",
    "text = \"Buffer overflow Buffer overflow Buffer overflow Buffer overflow Buffer overflow Buffer overflow Buffer overflow\"#\"Buffer overflow, in Solaris x86 mkcookie allows local users to obtain root access.\"\n",
    "text = \"Buffer overflow\"\n",
    "text2 = \"in Solaris x86 mkcookie allows local users to obtain root access.\"\n",
    "# text = text.split(\" \")\n",
    "# text = deal_with_text(text)\n",
    "# text = \"via a UDP packet, \"\n",
    "# text2 = \"resulting in Denial of Service.handle_ipDefaultTTL in agent/mibgroup/ip-mib/ip_scalars.c in Net-SNMP 5.8 through 5.9.3 has a NULL Pointer Exception bug that can be used by a remote attacker (who has write access) to cause the instance to crash via a crafted UDP packet, resulting in Denial of Service.\"\n",
    "\n",
    "text = \"via packet injection or crafted capture file,\"\n",
    "text2 = \"Memory leak in the NFS dissector in Wireshark 4.0.0 to 4.0.2 and 3.6.0 to 3.6.10 and allows denial of service via packet injection or crafted capture file\"\n",
    "\n",
    "text = \"via a crafted ZIP archive.\"\n",
    "text2 = \",loadAsync in JSZip before 3.8.0 allows Directory Traversal via a crafted ZIP archive.\"\n",
    "\n",
    "text = \"via internal API endpoints.,\"\n",
    "text2 = \"The iot-manager microservice 1.0.0 in Northern.tech Mender Enterprise before 3.2.2 allows SSRF because the Azure IoT Hub integration provides several SSRF primitives that can execute cross-tenant actions via internal API endpoints.\"\n",
    "\n",
    "# text = \"A local attacker \"\n",
    "# text2 = \"A logic issue was addressed with improved state management. This issue is fixed in iTunes 12.12.4 for Windows. A local attacker may be able to elevate their privileges.\"\n",
    "\n",
    "print(text)\n",
    "print(text2)\n",
    "a = get_sentence_embedding(text, text2)\n",
    "\n",
    "# print(a.shape)\n",
    "#\n",
    "#list转tensor\n",
    "# print(a.shape)\n",
    "# a = torch.tensor(a)\n",
    "# #把第二维度删除\n",
    "# a = torch.squeeze(a)\n",
    "#取平均值\n",
    "#把第一个维度删除\n",
    "\n",
    "\n",
    "a = torch.squeeze(a, 0)\n",
    "# print(a.shape)\n",
    "\n",
    "a = torch.mean(a, 0)\n",
    "a = torch.mean(a[:3], 0 )*0.5 + torch.mean(a[3:], 0)*0.5\n",
    "# print(a.shape)\n",
    "# a = torch.mean(a, 0)\n",
    "\n",
    "path_v_type_description = \"/home/user/Programs/PMA_lihang/mypaper/label_description/att_vec.txt\"#v_type.txt\"\n",
    "f_v_type = open(path_v_type_description, 'r', encoding='utf-8')\n",
    "f_v_type_reader = f_v_type.readlines()\n",
    "v_type_description = []\n",
    "for line in f_v_type_reader:\n",
    "    v_type_description.append(line.strip())\n",
    "\n",
    "for i in v_type_description:\n",
    "    print(i)\n",
    "    # print(i.split(\"##\"))\n",
    "    # text = i.split(\" \")\n",
    "    # text = get_sentence_embedding(text)\n",
    "    # a = get_vector(text)\n",
    "\n",
    "    # t1 = i.split(\"##\")[0]\n",
    "    # t2 = i.split(\"##\")[1]\n",
    "\n",
    "\n",
    "    # b = get_sentence_embedding(\"\",i)\n",
    "    b = get_sentence_embedding(i,\"\")\n",
    "\n",
    "\n",
    "    b = torch.squeeze(b, 0)\n",
    "\n",
    "\n",
    "    #list转tensor\n",
    "    # b = torch.tensor(b)\n",
    "    # #把第二维度删除\n",
    "    # b = torch.squeeze(b)\n",
    "    #取平均值\n",
    "    b = torch.mean(b, 0)\n",
    "    #取加权平均值，长度的前20%的token有80%的权重，长度的后80%的token有20%的权重\n",
    "    # b = torch.mean(b[0:int(len(b)*0.2)], 0) * 0.8 + torch.mean(b[int(len(b)*0.2):len(b)], 0) * 0.2\n",
    "    #\n",
    "    # b = torch.mean(b[:5], 0 )*0.8 + torch.mean(b[5:], 0)*0.2\n",
    "    # print(b.shape)\n",
    "\n",
    "    print(cos_sim(a, b))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.85601090e-17  3.19385854e-16  8.84749529e-01  7.88542957e-16\n",
      "   8.84749529e-01]\n",
      " [ 2.40164470e-17 -2.85484244e-16 -3.59240466e-16  1.00000000e+00\n",
      "  -1.88225279e-16]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# define a sample corpus of four texts\n",
    "corpus = [\"I like strawberries\", \"I love apples\", \"He hates bananas\", \"She enjoys oranges\",\"He hates baseball\"]\n",
    "\n",
    "# create a tf-idf matrix from the corpus\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# apply LSA model to reduce dimensionality to 2 topics\n",
    "lsa_model = TruncatedSVD(n_components=2)\n",
    "lsa_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "# get the topic-document matrix (each column is a topic vector for a text)\n",
    "td_matrix = lsa_matrix.T\n",
    "\n",
    "print(td_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/user/Programs/PMA_lihang/mypaper/module/bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1045, 2066, 18108, 1012, 102, 18108, 2024, 1037, 2785, 1997, 5909, 1012, 102]\n",
      "torch.Size([1, 14, 768])\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# 导入bert模型和tokenizer\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "path = \"/home/user/Programs/PMA_lihang/mypaper/module/bert-base-uncased\"\n",
    "#加载bert模型\n",
    "model = BertModel.from_pretrained(path)\n",
    "#加载分词器\n",
    "tokenizer = BertTokenizer.from_pretrained(path)\n",
    "\n",
    "# 定义两句话\n",
    "text1 = \"I like apples.\"\n",
    "text2 = \"Apples are a kind of fruit.\"\n",
    "\n",
    "# 使用tokenizer将文本转换为token id，并添加特殊符号[CLS]和[SEP]\n",
    "input_ids = tokenizer.encode(text1, text2)\n",
    "print(input_ids)\n",
    "\n",
    "# 输出：[101, 1045, 2066, 6207, 1012, 102, 6207, 2024, 1037, 2785, 1997, 2609, 1012, 102]\n",
    "#list转换为tensor\n",
    "input_ids = torch.tensor(input_ids).unsqueeze(0) # (batch_size=1 x sequence_length)\n",
    "# 使用model得到每个token和整个序列的向量表示\n",
    "outputs = model(input_ids)\n",
    "sequence_output = outputs.last_hidden_state # (batch_size=1 x sequence_length x hidden_size=768)\n",
    "pooled_output = outputs.pooler_output # (batch_size=1 x hidden_size=768)\n",
    "\n",
    "print(sequence_output.shape) # torch.Size([1 ,14 ,768])\n",
    "print(pooled_output.shape) # torch.Size([1 ,768])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
