{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载模型太慢了。。。单独拿出来\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import gensim\n",
    "\n",
    "\n",
    "# path = \"/home/user/Programs/PMA_lihang/mypaper/module/bert_large_uncased\"\n",
    "# #加载bert模型\n",
    "# model = BertModel.from_pretrained(path)\n",
    "# #加载分词器\n",
    "# tokenizer = BertTokenizer.from_pretrained(path)\n",
    "\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    " \n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\"/home/user/Programs/PMA_lihang/code_lihang/word2vec/model/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "# model = Word2Vec.load(\"/home/user/Programs/PMA_lihang/code_lihang/word2vec/model/word2vec.model\")\n",
    "#加载谷歌新闻预训练的word2vec模型\n",
    "# model = Word2Vec.load(\"/home/user/Programs/PMA_lihang/code_lihang/word2vec/model/GoogleNews-vectors-negative300.bin\")\n",
    "# model = Word2Vec.load(\"/home/user/Programs/PMA_lihang/code_lihang/word2vec/model/GoogleNews-vectors-negative300.bin\", binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import gensim\n",
    "\n",
    "\n",
    "# path = \"/home/user/Programs/PMA_lihang/mypaper/module/bert-base-uncased\"\n",
    "# #加载bert模型\n",
    "# model = BertModel.from_pretrained(path)\n",
    "# #加载分词器\n",
    "# tokenizer = BertTokenizer.from_pretrained(path)\n",
    "\n",
    "# for param in model.parameters():\n",
    "#     param.requires_grad = False\n",
    " \n",
    "# model = gensim.models.KeyedVectors.load_word2vec_format(\"/home/user/Programs/PMA_lihang/code_lihang/word2vec/model/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "# model = Word2Vec.load(\"/home/user/Programs/PMA_lihang/code_lihang/word2vec/model/word2vec.model\")\n",
    "#加载谷歌新闻预训练的word2vec模型\n",
    "# model = Word2Vec.load(\"/home/user/Programs/PMA_lihang/code_lihang/word2vec/model/GoogleNews-vectors-negative300.bin\")\n",
    "# model = Word2Vec.load(\"/home/user/Programs/PMA_lihang/code_lihang/word2vec/model/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "from gensim.parsing.preprocessing import STOPWORDS \n",
    "stop_words = STOPWORDS\n",
    "\n",
    "\n",
    "def deal_with_text(v_type):\n",
    "    v_type_ = []\n",
    "    for i in range(len(v_type)):\n",
    "        content = v_type[i]\n",
    "        content = re.sub(\"[^\\u4e00-\\u9fa5^a-z^A-Z^0-9]\", \" \", str(content))\n",
    "        # 只保留数字和字母\n",
    "        content = re.sub(\"[^a-z^A-Z^0-9]\", \" \", str(content))\n",
    "        #去除连续的空格\n",
    "        content = re.sub(\"\\s+\", \" \", content)\n",
    "        #删除回车符\n",
    "        content = re.sub(\"\\n\", \" \", content)\n",
    "        #删除开头和结尾的空格\n",
    "        content = content.strip()\n",
    "        #print(content)\n",
    "        content = content.split(\" \")\n",
    "        #将所有大写字母转换为小写\n",
    "        content = [word.lower() for word in content]\n",
    "        for j in range(len(content)):\n",
    "            v_type_.append(content[j])\n",
    "            # print(\"处理后的文本：\")\n",
    "            # print(content[j])\n",
    "        # print(v_type_)\n",
    "\n",
    "         \n",
    "        # 定义一个文本字符串 \n",
    "        # text = \"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and fishery rihgts at once.\" \n",
    "        # # 对文本进行分词（默认使用空格分隔）\n",
    "        # tokens = text.split()\n",
    "        # # 遍历分词结果，删除在停用词列表中的词 \n",
    "        # filtered_tokens = [token for token in v_type_ if token not in stop_words] \n",
    "        # # 将过滤后的分词结果重新拼接成字符串 \n",
    "        \n",
    "        # filtered_text = ' '.join(filtered_tokens)\n",
    "        # v_type_ = filtered_text.split(\" \")\n",
    "        # print(v_type_)\n",
    "    return v_type_\n",
    "\n",
    "\n",
    "\n",
    "def get_sentence_embedding(text, text2):\n",
    "    # tokenize text and convert tokens to ids\n",
    "    tokens = tokenizer.tokenize(text) # ['i', 'like', 'strawberries']\n",
    "    print(tokens)\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens) # [1045, 2066, 13137]\n",
    "\n",
    "    tokens2 = tokenizer.tokenize(text2) # ['i', 'like', 'strawberries']\n",
    "    print(tokens2)\n",
    "    input_ids2 = tokenizer.convert_tokens_to_ids(tokens2) # [1045, 2066, 13137]\n",
    "    \n",
    "    # print(input_ids)\n",
    "    # print(input_ids2)\n",
    "\n",
    "    # add special tokens [CLS] and [SEP]\n",
    "    input_ids = [tokenizer.cls_token_id] + input_ids + [tokenizer.sep_token_id] + input_ids2 + [tokenizer.sep_token_id] # [101, 1045, 2066, 13137, 102]\n",
    "        # [101, 1045, 2066, 13137, 102]\n",
    "\n",
    "    # convert input ids to tensor\n",
    "    input_tensor = torch.tensor([input_ids]) # shape (1,5)\n",
    "\n",
    "    # print(input_tensor.shape)   \n",
    "    # feed input tensor to BERT model\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "\n",
    "    \n",
    "    # get the last hidden states of BERT\n",
    "    last_hidden_state = output.last_hidden_state # shape (1,5,dim)\n",
    "    # print(last_hidden_state.shape)\n",
    "    # get the sentence embedding by taking the first token ([CLS])\n",
    "    sentence_embedding = last_hidden_state#[:,0,:] # shape (1,dim)\n",
    "    # print(sentence_embedding.shape)\n",
    "    return sentence_embedding\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(len(v_type_description) - 1):\n",
    "#     print(v_type_description[i])\n",
    "\n",
    "#计算两个tensor的余弦相似度，shape是(1,768)\n",
    "def cos_sim(a, b):\n",
    "    cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "    cos_sim = cos(a, b)\n",
    "    return cos_sim\n",
    "\n",
    "#计算两个tensor的欧式距离，shape是(1,768)\n",
    "def euclidean_dist(a, b):\n",
    "    dist = nn.PairwiseDistance(p=2)\n",
    "    dist = dist(a, b)\n",
    "    return dist\n",
    "\n",
    "#计算两个tensor的曼哈顿距离，shape是(1,768)\n",
    "def manhattan_dist(a, b):\n",
    "    dist = nn.PairwiseDistance(p=1)\n",
    "    dist = dist(a, b)\n",
    "    return dist\n",
    "\n",
    "#定义\n",
    "# a = get_sentence_embedding(text)\n",
    "# for i in v_type_description:\n",
    "#     b = get_sentence_embedding(i)\n",
    "#     print(cos_sim(a, b))\n",
    "\n",
    "# a = get_sentence_embedding(text)\n",
    "# for i in v_type_description:\n",
    "#     b = get_sentence_embedding(i)\n",
    "#     print(euclidean_dist(a, b))\n",
    "\n",
    "# a = get_sentence_embedding(text)\n",
    "# for i in v_type_description:\n",
    "#     b = get_sentence_embedding(i)\n",
    "#     print(manhattan_dist(a, b))\n",
    "\n",
    "#自己训练的word2vec\n",
    "def get_vectorOOV1(s):\n",
    "  try:\n",
    "    return np.array(model.wv.get_vector(s))\n",
    "  except KeyError:\n",
    "    #print(\"$$$$$$$\")\n",
    "    return np.random.random((300,))\n",
    "\n",
    "def get_vector(v_type_):\n",
    "    v_type_vec=[]\n",
    "    for i in range(len(v_type_)):\n",
    "        temp = []\n",
    "        #for j in range(len(v_type_[i])):\n",
    "        temp.append(get_vectorOOV1(v_type_[i]))\n",
    "        # for j in range(10-len(v_type_[i])):\n",
    "        #     temp.append(np.random.random((300,)))\n",
    "        v_type_vec.append(temp)\n",
    "    return v_type_vec\n",
    "\n",
    "#新闻word2vec\n",
    "def get_vectorOOV2(s):\n",
    "  try:\n",
    "    return np.array(model.get_vector(s))\n",
    "  except KeyError:\n",
    "    #print(\"$$$$$$$\")\n",
    "    return np.random.random((300,))\n",
    "\n",
    "def get_vector2(v_type_):\n",
    "    v_type_vec=[]\n",
    "    for i in range(len(v_type_)):\n",
    "        temp = []\n",
    "        #for j in range(len(v_type_[i])):\n",
    "        temp.append(get_vectorOOV2(v_type_[i]))\n",
    "        # for j in range(10-len(v_type_[i])):\n",
    "        #     temp.append(np.random.random((300,)))\n",
    "        v_type_vec.append(temp)\n",
    "    return v_type_vec\n",
    "\n",
    "# 导入WordNetLemmatizer和word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# 创建一个WordNetLemmatizer对象\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def stemSentence(words):\n",
    "    for i in range(len(words)):\n",
    "        words[i] = lemma.lemmatize(words[i])\n",
    "    return words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完数据，开始循环\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#用word2vec模型 分类 att_type 自身的描述 \n",
    "\n",
    "from gensim.parsing.preprocessing import STOPWORDS \n",
    "import numpy \n",
    "stop_words = STOPWORDS \n",
    "# 定义一个文本字符串 \n",
    "def filter(tokens):\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words and len(token) > 1] \n",
    "    return filtered_tokens\n",
    "# text = \"Buffer overflow Buffer overflow Buffer overflow Buffer overflow Buffer overflow Buffer overflow Buffer overflow\"#\"Buffer overflow, in Solaris x86 mkcookie allows local users to obtain root access.\"\n",
    "# text = \"Buffer overflow, in Solaris x86 mkcookie allows local users to obtain root access.\"\n",
    "# text1 = \"attackers with Overall/Read permission\"\n",
    "# text2 = \"The Remote Data Service Object (RDS.DataControl) in Microsoft Internet Explorer 6 on Windows 2000 allows remote attackers to cause a denial of service (crash) via a series of operations that result in an invalid length calculation when using SysAllocStringLen, then triggers a buffer over-read.\"\n",
    "\n",
    "# text = \"an attacker, A flaw was found in the hivex library. This flaw allows to input a specially crafted Windows Registry (hive) file, which would cause hivex to recursively call the _get_children() function, leading to a stack overflow. The highest threat from this vulnerability is to system availability.\"\n",
    "# text = \"A  attacker,An out-of-bounds write issue was addressed with improved bounds checking. This issue is fixed in macOS Monterey 12.3. A remote attacker may be able to cause unexpected system termination or corrupt kernel memory.\"\n",
    "#text = \"A attacker ,A logic issue was addressed with improved state management. This issue is fixed in iTunes 12.12.4 for Windows. A attacker may be able to elevate their privileges.\"\n",
    "# tetx = \"an attacker, ASP.NET Core 2.0 allows to steal log-in session information such as cookies or authentication tokens via a specially crafted URL aka \"\"ASP.NET Core Elevation Of Privilege Vulnerability\"\".\"\n",
    "\n",
    "# text = \"via a UDP packet, resulting in Denial of Service.handle_ipDefaultTTL in agent/mibgroup/ip-mib/ip_scalars.c in Net-SNMP 5.8 through 5.9.3 has a NULL Pointer Exception bug that can be used by a remote attacker (who has write access) to cause the instance to crash via a crafted UDP packet, resulting in Denial of Service.\"\n",
    "def get_similar_score(text1,text2):\n",
    "    # print(\"函数中的\")\n",
    "    # print(text1)\n",
    "    # print(text2)\n",
    "    text = text1.split(\" \")\n",
    "\n",
    "    # print(text1)\n",
    "    text = deal_with_text(text)\n",
    "    text = filter(text)\n",
    "    text = stemSentence(text)\n",
    "    # text1[0] = \"remote\"\n",
    "    # print(text1)\n",
    "    aspect = get_vector2(text)\n",
    "    #list转tensor\n",
    "    # print(len(a[0][0]))\n",
    "    aspect = torch.tensor(aspect)\n",
    "    #把第二维度删除\n",
    "    aspect = torch.squeeze(aspect)\n",
    "    #如果维度为300，则在第一维度增加一个维度\n",
    "    if aspect.shape[0] == 300:#只有一个词，则相似度忽略，凭借其他方面判断\n",
    "        return torch.tensor(numpy.array(0.0))\n",
    "        # print(\"#######\")\n",
    "        aspect = aspect.unsqueeze(0)\n",
    "\n",
    "    # print(aspect.shape)\n",
    "    #取平均值\n",
    "    if aspect.shape[0] == 1:\n",
    "        aspect2 = torch.mean(aspect, 0)#att_type\n",
    "    # if aspect.shape[0] > 1:\n",
    "    #     aspect = torch.mean(aspect[:1], 0 )*0.2 + torch.mean(aspect[1:], 0)*0.8\n",
    "    #     aspect2 = torch.mean(aspect[:1], 0 )*0.8 + torch.mean(aspect[1:], 0)*0.2\n",
    "    # else:\n",
    "    #     aspect = torch.mean(aspect, 0)\n",
    "    #     aspect2 = torch.mean(aspect, 0)\n",
    "    # print(a.shape)\n",
    "    else:\n",
    "    # aspect = torch.mean(aspect[:1], 0 )*0.2 + torch.mean(aspect[1:], 0)*0.8\n",
    "        aspect2 = torch.mean(aspect[:1], 0 )*0.8 + torch.mean(aspect[1:], 0)*0.2#att_type\n",
    "        #后半部分\n",
    "        #middle = int(aspect.shape[0]/2)\n",
    "        #为了处理execute arbitrary code or cause a denial of service (memory corruption)，后半部分的相似度\n",
    "        #初始化aspect2_\n",
    "\n",
    "        flag = False\n",
    "        if aspect.shape[0] > 5:\n",
    "            flag = True\n",
    "            aspect2_ = torch.mean(aspect[3:4], 0 )*0.8 + torch.mean(aspect[4:], 0)*0.2#att_type  \n",
    "        \n",
    "\n",
    "    text = text2.split(\" \")\n",
    "    \n",
    "    text = deal_with_text(text)\n",
    "    text = filter(text)\n",
    "    text = stemSentence(text)\n",
    "    # print(text)\n",
    "    # text[0] = \"remote\"\n",
    "    # print(text)\n",
    "    # a = get_vector(text)\n",
    "    label = get_vector2(text)\n",
    "    \n",
    "    \n",
    "    #list转tensor\n",
    "    label = torch.tensor(label)\n",
    "    #把第二维度删除\n",
    "    label = torch.squeeze(label)\n",
    "    # print(label.shape)\n",
    "    # print(\"^^^^^^^^^^^^^^^^^^^^\")\n",
    "    # print(label.shape[0])\n",
    "    if label.shape[0] == 300:\n",
    "        # print(\"********\")\n",
    "        label = label.unsqueeze(0)\n",
    "    # print(label.shape)\n",
    "    #取平均值\n",
    "    if label.shape[0] == 1:\n",
    "        label2 = torch.mean(label, 0)\n",
    "    # 取加权平均值，长度的前20%的token有80%的权重，长度的后80%的token有20%的权重\n",
    "    # b = torch.mean(b[0:int(len(b)*0.2)], 0) * 0.8 + torch.mean(b[int(len(b)*0.2):len(b)], 0) * 0.2\n",
    "    #\n",
    "    # if label.shape[0] > 1:\n",
    "    #     print(\"here\",label.shape[0])\n",
    "    #     label = torch.mean(label[:1], 0 )*0.2 + torch.mean(label[1:], 0)*0.8\n",
    "    #     label2 = torch.mean(label[:1], 0 )*0.8 + torch.mean(label[1:], 0)*0.2\n",
    "    # else:\n",
    "    #     label = torch.mean(label, 0)\n",
    "    #     label2 = torch.mean(label, 0)\n",
    "\n",
    "    # label = torch.mean(label[:1], 0 )*0.2 + torch.mean(label[1:], 0)*0.8\n",
    "    else:\n",
    "        label2 = torch.mean(label[:1], 0 )*0.8 + torch.mean(label[1:], 0)*0.2\n",
    "\n",
    "    # print(b.shape)\n",
    "    # print(cos_sim(aspect, label))\n",
    "    # print(euclidean_dist(aspect, label))\n",
    "    # print(label == aspect)\n",
    "    # print(manhattan_dist(aspect, label))\n",
    "\n",
    "    # s1 = cos_sim(aspect, label)\n",
    "    s2 = cos_sim(aspect2, label2)\n",
    "    # print(type(s1),type(s2))\n",
    "    if flag:\n",
    "        # print(\"执行比较后半部分\")\n",
    "        s2_ = cos_sim(aspect2_, label2)\n",
    "        if s2 < s2_:\n",
    "            return s2_\n",
    "    # print(\"s1s2\",s1.shape,s2.shape)\n",
    "    # if s1 > s2:\n",
    "    #     return s1\n",
    "    # else:\n",
    "    #     return s2\n",
    "    # print(\"sim:\",s1)\n",
    "    # print(text1,text2)\n",
    "    # print(\"描述、标签sim:\",s2)\n",
    "    \n",
    "    return s2\n",
    "\n",
    "# path_att_type = \"/home/user/Programs/PMA_lihang/mypaper/module/temp.txt\"\n",
    "path_att_type = \"/home/user/Programs/PMA_lihang/mypaper/module/temp_impact.txt\"\n",
    "# path_v_type_description = \"/home/user/Programs/PMA_lihang/mypaper/label_description/att_type_aspect_context.txt\"#v_type.txt\"\n",
    "path_v_type_description = \"/home/user/Programs/PMA_lihang/mypaper/label_description/att_impact_aspect.txt\"\n",
    "f_v_type = open(path_v_type_description, 'r', encoding='utf-8')\n",
    "f_v_type_reader = f_v_type.readlines()#\n",
    "\n",
    "v_type_description = []#我对标签的扩充描述\n",
    "# ii = 0\n",
    "\n",
    "f_att_type = open(path_att_type, 'r', encoding='utf-8')\n",
    "f_att_type_reader = f_att_type.readlines()\n",
    "att_type_description = []#数据中的方面描述\n",
    "\n",
    "\n",
    "ii = 0\n",
    "count = [] #统计每个类型出现的次数 改成用字典来存 描述和数量\n",
    "\n",
    "for line in f_att_type_reader:\n",
    "    # contens = line.strip().split(\" ## \")\n",
    "\n",
    "    att_type_description.append(line.strip().split(\"\\t\")[0])\n",
    "\n",
    "    count.append(int(line.strip().split(\"\\t\")[-1]))\n",
    "    #处理前几个高频出现的类型\n",
    "\n",
    "\n",
    "    #下次运行，把下面的代码注释，要遍历所有的数据，效果会更好\n",
    "    ii += 1\n",
    "    if ii == 400:\n",
    "        break\n",
    "\n",
    "# print(v_type_description)\n",
    "\n",
    "import csv\n",
    "# path  = \"/home/user/Programs/PMA_lihang/mypaper/module/temp1.txt\"\n",
    "path  = \"/home/user/Programs/PMA_lihang/mypaper/module/temp1_impact.txt\"\n",
    "f = open(path, 'a', encoding='utf-8')\n",
    "\n",
    "def compare_first():#\n",
    "    #只取第一个\n",
    "    for line in f_v_type_reader: #\n",
    "        # contens = line.strip().split(\" ## \")\n",
    "        v_type_description.append(line.strip().split(\"##\")[0])\n",
    "        # ii += 1\n",
    "\n",
    "    for text1 in att_type_description:\n",
    "        kk = 1\n",
    "        \n",
    "        for i in v_type_description:\n",
    "            \n",
    "            #将kk写入txt文件\n",
    "            f.write(str(kk) + \"\\n\")\n",
    "            f.write(str(text1) + \"\\n\")\n",
    "            f.write(str(i) + \"\\n\")\n",
    "\n",
    "            # print(text1)\n",
    "            # print(i)\n",
    "\n",
    "            score = get_similar_score(text1,i)\n",
    "            #tensor转float\n",
    "            # score.item()\n",
    "\n",
    "            f.write(str(score) + \"\\n\")\n",
    "            kk += 1\n",
    "        #写入文件\n",
    "# compare_first()\n",
    "\n",
    "\n",
    "def compare_all():#比较各个描述与所有标签的相似度\n",
    "\n",
    "    \n",
    "    iikk = 0    \n",
    "    for text1 in att_type_description:#对于一条数据描述\n",
    "        print(iikk)\n",
    "        iikk += 1\n",
    "        if iikk == 20:\n",
    "            break\n",
    "        # print(text1)\n",
    "        \n",
    "\n",
    "        candidate = []#一个类别的所有描述  \n",
    "        kk = 0\n",
    "        for line in f_v_type_reader:#取一行标签扩充的描述\n",
    "            #第几类\n",
    "\n",
    "            # print(line)\n",
    "            #每次的一行只是一个类别\n",
    "            \n",
    "            candidate = line.split(\" ## \") \n",
    "            # candidate = list(filter(candidate))\n",
    "            candidate = [x for x in candidate if x != '' and x != '\\n']\n",
    "            #定义最小的tensor相似度为-1\n",
    "            sim = torch.tensor(numpy.array(-1.0))\n",
    "            f.write(\"序号\"+str(kk) + \"\\n\")\n",
    "            f.write(\"文本\"+str(text1) + \"\\n\")\n",
    "            # f.write(str(candidate[0]) + \"\\n\")\n",
    "            # print(\"***********\")\n",
    "            # print(candidate)\n",
    "            #获取最相似的\n",
    "            ii = 0\n",
    "            kk += 1\n",
    "\n",
    "            for text2 in candidate:#对于一个标签的每个描述\n",
    "                # f.write(str(text2) + \"\\n\")\n",
    "                # print(i)\n",
    "                #将kk写入txt文件\n",
    "                # print(\"输入的两个文本：\")\n",
    "                # print(type(text1))\n",
    "                # print(type(text2))\n",
    "                #取最大的\n",
    "                \n",
    "                s = get_similar_score(str(text1),str(text2))\n",
    "                \n",
    "                # print(\"&&&&&&&&&&&&&&\",text2.strip() ,\"&&&&&&&&&&&&&\")\n",
    "                f.write(\"标签\"+text2.strip() + \"\\n\")\n",
    "                # f.write(\"\\n\")\n",
    "                f.write(\"得分\"+str(s) +\"\\n\")\n",
    "                # f.write( \"\\n\")\n",
    "                # print(type(s))\n",
    "                # print(\"得到的相似度：\")\n",
    "                # print(s)\n",
    "                if sim < s:\n",
    "                    sim = s\n",
    "                    jj = ii\n",
    "                # sim = max(sim, get_similar_score(text1,i).item())\n",
    "                #score = get_similar_score(text1,i)\n",
    "                #tensor转float\n",
    "                # score.item()\n",
    "                \n",
    "                ii += 1\n",
    "            # f.write(str(\"****最大的项******\" + \"\\n\"))\n",
    "            f.write(\"**********最大的项************\"+\"\\n\")\n",
    "            f.write(candidate[jj] + str(sim) + \"\\n\")\n",
    "            # print(\"sim: \",sim)\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "            # print(candidate)\n",
    "        f.write(\"####################################\")\n",
    "print(\"处理完数据，开始循环\")\n",
    "compare_all()\n",
    "\n",
    "import time\n",
    "list_all = []\n",
    "count_all = []\n",
    "def get_class_num():#打印各个类别的实际中的描述\n",
    "    #一共6个类别\n",
    "    # for i in range(7):\n",
    "    for i in range(10):\n",
    "        list_all.append([])\n",
    "        count_all.append(0)\n",
    "    #所有的描述\n",
    "    iii = 0\n",
    "    for text1 in att_type_description: # 数据中的描述\n",
    "        time.sleep(0.1)\n",
    "        # print(text1)\n",
    "        kk = 1\n",
    "        #类别index\n",
    "        # class_num = -1\n",
    "        class_num = []\n",
    "\n",
    "        max_sim = -1\n",
    "        #获取类别\n",
    "        class_now = 0\n",
    "        for line in f_v_type_reader:\n",
    "            # print(line)\n",
    "            #每次的一行只是一个类别\n",
    "            candidate = line.split(\" ## \")\n",
    "            # candidate = list(filter(candidate))\n",
    "            candidate = [x for x in candidate if x != '' and x != '\\n']\n",
    "            #定义最小的tensor相似度为-1\n",
    "            sim = torch.tensor(numpy.array(-1.0))\n",
    "            # f.write(\"序号\"+str(kk) + \"\\n\")\n",
    "            # f.write(\"文本\"+str(text1) + \"\\n\")\n",
    "            # f.write(str(candidate[0]) + \"\\n\")\n",
    "            # print(\"***********\")\n",
    "            # print(candidate)\n",
    "            #获取最相似的\n",
    "            ii = 0\n",
    "            for text2 in candidate:\n",
    "                # f.write(str(text2) + \"\\n\")\n",
    "                # print(i)\n",
    "                #将kk写入txt文件\n",
    "                # print(\"输入的两个文本：\")\n",
    "                # print(type(text1))\n",
    "                # print(type(text2))\n",
    "                #取最大的\n",
    "                \n",
    "                s = get_similar_score(str(text1),str(text2))\n",
    "                \n",
    "                # print(\"&&&&&&&&&&&&&&\",text2.strip() ,\"&&&&&&&&&&&&&\")\n",
    "                # f.write(\"标签\"+text2.strip() + \"\\n\")\n",
    "                # f.write(\"\\n\")\n",
    "                # f.write(\"得分\"+str(s) +\"\\n\")\n",
    "                # f.write( \"\\n\")\n",
    "                # print(type(s))\n",
    "                # print(\"得到的相似度：\")\n",
    "                # print(s)\n",
    "                if sim < s:\n",
    "                    sim = s\n",
    "                    jj = ii\n",
    "                    \n",
    "                # sim = max(sim, get_similar_score(text1,i).item())\n",
    "                #score = get_similar_score(text1,i)\n",
    "                #tensor转float\n",
    "                # score.item()\n",
    "                \n",
    "                ii += 1\n",
    "                # 临时加的\n",
    "                if s > 0.9:\n",
    "                    class_num.append(class_now)\n",
    "                    break\n",
    "\n",
    "            if max_sim < sim:\n",
    "                max_sim = sim\n",
    "                #class_num = class_now\n",
    "\n",
    "            # f.write(str(\"****最大的项******\" + \"\\n\"))\n",
    "            # f.write(\"**********最大的项************\"+\"\\n\")\n",
    "            # f.write(candidate[jj] + str(sim) + \"\\n\")\n",
    "            # print(\"sim: \",sim)\n",
    "            # f.write(\"\\n\")\n",
    "            kk += 1\n",
    "            # print(candidate)\n",
    "\n",
    "            class_now += 1\n",
    "\n",
    "        if max_sim < 0.9:\n",
    "            # class_num = 6\n",
    "            # class_num = 9\n",
    "            class_num.append(9)\n",
    "        \n",
    "        # list_all[class_num].append(text1)\n",
    "        # #统计类别的总数\n",
    "        # count_all[class_num] += count[iii]\n",
    "        # iii += 1\n",
    "\n",
    "        # 处理这条数据所属的多个类别\n",
    "        for i in class_num:\n",
    "            list_all[i].append(text1)\n",
    "            #统计类别的总数\n",
    "            # count_all[i] += count[iii]\n",
    "            # iii += 1\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "#统计每个类别的数量\n",
    "# path = \"/home/user/Programs/PMA_lihang/mypaper/module/temp2.txt\"\n",
    "# path = \"/home/user/Programs/PMA_lihang/mypaper/module/temp2_impact.txt\"\n",
    "# f = open(path, 'w', encoding='utf-8')\n",
    "# get_class_num()\n",
    "# for i in list_all:\n",
    "#     for j in i:\n",
    "#         f.write(j + \"\\n\")\n",
    "#     f.write(\"******************\\n\")\n",
    "# f.close()\n",
    "# # count_all[6] += 7500\n",
    "# for i in count_all:\n",
    "#     print(i)   \n",
    "#将分类的情况按照类别写入文件\n",
    "#建一个列表，每个元素是一个类别，每个类别是一个列表\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "执行比较后半部分\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "s1 = \"execute arbitrary code or cause a denial of service (memory corruption)\"\n",
    "s2 = \"cause a denial of service (memory corruption)\"\n",
    "\n",
    "score = get_similar_score(s1,s2)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78440\n",
      "10184\n",
      "11872\n",
      "1362\n",
      "3276\n",
      "584\n",
      "24752\n",
      "1174\n",
      "0\n",
      "0\n",
      "722\n",
      "7580\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/user/Programs/PMA_lihang/mypaper/module/temp2_impact.txt\"\n",
    "f = open(path, 'w', encoding='utf-8')\n",
    "get_class_num()\n",
    "for i in list_all:\n",
    "    for j in i:\n",
    "        f.write(j + \"\\n\")\n",
    "    f.write(\"******************\\n\")\n",
    "f.close()\n",
    "# count_all[6] += 7500\n",
    "for i in count_all:\n",
    "    print(i)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
